[{"content":"不同於Paxos，Raft使用Leader(領導者)、Follower(追隨者)等更直觀的術語 並且簡化了複雜的流程，主要還是有三個流程\nLeader Election Log Replication Log Re 在Raft中，有以下三個角色代表不同節點 1. Leader Election 初始化狀態 ： - 系統中的所有節點開始時都處於Follower狀態。 超時觸發選舉 ： - 每個Follower節點在一定時間內沒有收到來自Leader的心跳訊號(Heartbeat)，它會轉變為Candidate並發起選舉， 以下舉例為節點初始化時的狀態(沒任何Leader 傳 heaerbeat)，每個節點的進度條則代表沒收到心跳訊號的時間 發送投票請求 ： - Candidate節點向其他所有節點發送RequestVote請求，並附帶其當前的日誌索引和任期號。 接受投票 ： - 其他節點（Followers）收到RequestVote請求後，會根據Candidate的日誌索引和任期號決定是否投票\n如果該Candidate的日誌比自己更新，且尚未投票給其他Candidate，則會投票給該Candidate。\n這裡是Raft算法的核心之一，透過網路時間差來投票選出Leader，像上面gif顯示的結尾部分，Candidate變成Leader 當選為Leader ： - Candidate節點如果獲得多數節點的投票（超過半數），則成為Leader。 當選後，它會立即向其他節點發送心跳訊號，通知其成為新的Leader，以下為正常Leader持續傳送心跳訊號的樣子 處理失敗情況 ： - 如果Candidate在一定時間內沒有獲得足夠的投票，它會重新進入Follower狀態，並等待下一次選舉超時再次發起選舉。\n以下是當Leader掛掉 不再傳送心跳訊號時時，各節點會再重新選出新Leader的選舉機制（意義上就是回到 1.初始化狀態 開始)\t以下影片為多節點觸發投票機制的情況 Your browser doesn't support HTML5 video. Here is a link to the video instead. 2. Log Replication 日誌複製 當有了領導者之後，一旦系統有發生改變時，我們就需要將系統的所有變更複製到所有節點 日誌條目追加 ： 當 Leader 接收到一個新的數據變更請求(例如,增加一筆資料),它會將該變更記錄為一個新的日誌條目,並將其追加到其本地日誌中。\n以下例子 (綠色節點=Client) 是Client 傳一筆資料 = 5 的資料給 Leader\n發送 AppendEntries 請求 ： Leader 會並行地將新的日誌條目發送給所有 Follower 節點,通過發送 AppendEntries RPC 請求。每個 Follower 在收到 AppendEntries 請求後,會將新的日誌條目附加到其本地日誌中 回覆成功或失敗 ： Follower 處理完 AppendEntries 請求後,會向 Leader 回覆是否成功附加了新的日誌條目。 如果大多數(\u0026gt;50%)Follower 成功複製了日誌條目，則該條目被認為已經被認同了，並將回應傳送給Client端，狀態為已提交(Committed)。\n應用日誌條目 ： 一旦某個日誌條目被提交,Leader 會通知所有 Follower 應用該條目到狀態機器(State Machine)中。這樣,所有節點的數據狀態就保持一致。 處理網絡分區 ： Raft 甚至可以在網路分區時保持一致性 如果在日誌複製過程中出現網絡分區,導致 Leader 無法與部分 Follower 通信,則 Leader 會無限期等待這些 Follower 重新上線。一旦重新建立連接,Leader 會自動將它們的日誌複製過來。\n讓我們新增一個分割區來將 A 和 B 與 C、D 和 E 分開例子： 由於我們的分裂，我們現在有兩位不同任期的Leader 我們新增另一個客戶端並嘗試更新兩個Leader的資料 各分區開始各做各的\n圖中下面的一個Client端將嘗試將節點 B 的值設為『3』，但由於節點 B 因為節點數量不夠多而無法使選舉機制成功，所以無法複製資料到多數節點，因此其日誌條目一直保持未提交狀態\n(專注看下面的分區) 再來看上面的Client端分區 Client 將嘗試將節點E的值設為“8”，因為結點多到可以使選舉機制成功，所以其他Follwer也會更新\n(專注看上面的分區) 當我們修復網路分割區時\n節點 B 將會看到更新的『選舉任期』，所以無條件接受別人的資料版本，並接受新領導者的日誌，接著，我們的日誌在整個叢集中就變成是一致的了。 通過以上過程,Raft 確保了在任何時候,大多數節點的日誌都是完全一致的。這樣就保證了系統的數據一致性和容錯性。\n","date":"2022-03-06T00:00:00Z","image":"https://shawnswu.github.io/secondbrain/p/raft-algorithom/images/cover_hu686d1a75a874ecdd31b618ca4af7cce2_45228_120x120_fill_q75_box_smart1.jpeg","permalink":"https://shawnswu.github.io/secondbrain/p/raft-algorithom/","title":"Raft Algorithom"},{"content":" Imagine a scenario where we have a large number of machines, and our system needs to monitor the real-time status of each machine, which means the machines need to send data back every minute, or even every second. Sending data through a RESTful API in this situation isn\u0026rsquo;t efficient, So we need a Guy that can store all this data, keep it safe, and respond to requests whenever someone wants to query it. so this is why we need Kafka.\nWhy We Need Kafka Handling High Throughput of Data Kafka is designed to handle massive volumes of data with high throughput. . If your application generates or processes a large amount of data continuously, Kafka can efficiently manage this data flow without bottlenecks.\nEnsuring Data Consistency and Durability Kafka uses a log-based storage mechanism that ensures data consistency and durability. This means your data is reliably stored and can be replicated across multiple nodes, preventing data loss even.\nDecoupling Data Producers and Consumers One of Kafka’s key strengths is its ability to decouple data producers and consumers .\nProducers simply send data to Kafka without needing to know who will consume it or how it will be processed. Consumers pull data from Kafka as needed, allowing them to process data independently. This decoupling enhances system flexibility and makes it easier to scale and maintain.\nReal-Time Data Processing Kafka supports real-time data processing, making it ideal for applications that require immediate data analysis and action. For example, :\nMonitoring systems Anomaly detection Real-time analytics Kafka enables quick and efficient data handling.\nHigh Scalability and Fault Tolerance Kafka’s architecture is inherently scalable. You can add more brokers to handle increased load , kafka will automatically balance the data across the cluster.\nAdditionally, Kafka’s replication mechanism ensures high availability and fault tolerance, making it a reliable choice for mission-critical applications.\nEvent Sourcing and Stream Processing Kafka is perfect for event sourcing and stream processing architectures. It allows you to store each event as a record, enabling you to reconstruct the state of your application from these events. This is particularly useful in microservices architectures where maintaining the state across services is crucial.\nWhen You Might Use Kafka Real-Time Analytics When you need to analyze data in real-time, such as tracking user activity on a website, monitoring system performance, or financial transaction analysis, Kafka provides a robust platform to ingest, process, and analyze data instantly.\nData Integration Kafka is excellent for integrating data from various sources into a single platform. If you have multiple systems generating data, Kafka can aggregate this data in real-time, making it accessible for processing and analysis.\nLog Aggregation For applications that generate logs from different services or systems, Kafka can centralize these logs, making it easier to monitor and troubleshoot issues.\nMicroservices Communication In a microservices architecture, different services need to communicate and share data. Kafka acts as a central hub, facilitating seamless communication between services through its publish-subscribe model.\nData Lakes and Warehousing If you’re building a data lake or warehouse, Kafka can efficiently stream data from various sources into your storage systems, ensuring data is continuously updated and available for analysis.\nMessage Queuing For applications that require reliable and ordered message delivery, such as task scheduling or asynchronous processing, Kafka provides a durable and scalable message queuing solution.\nIoT Data Streams In IoT applications, devices generate continuous data streams that need to be processed in real-time. Kafka’s ability to handle high throughput and real-time processing makes it ideal for IoT data management.\nMy thoughts If your application involves high throughput data ingestion, real-time analytics, data integration, or microservices communication, Kafka is a solution worth considering. It provides the scalability, fault tolerance, and flexibility needed to handle modern data challenges effectively.\n","date":"2023-08-25T00:00:00Z","image":"https://shawnswu.github.io/secondbrain/p/getting-to-know-kafka-1/images/apache-kafka_hu148f63155e1096e2ff1607c98c1189e7_45286_120x120_fill_q75_box_smart1.jpg","permalink":"https://shawnswu.github.io/secondbrain/p/getting-to-know-kafka-1/","title":"Getting to Know Kafka (1)"},{"content":"What are Kafka characteristic? Horizontal Scalability and High Availability Achieved through partitioning and replication mechanisms.\nData Consistency : Ensured by using a log mechanism for data persistence and replication.\nSeparation of Data Storage and Consumption : Producers generate data while consumers read and process data independently, enhancing system flexibility.\nDecoupling Producers and Consumers : Producers send data to Kafka without worrying about the consumers. Consumers pull data from Kafka as needed, known as the pull model, which decouples the dependency between the two and increases system flexibility.\nKafka is Pull Model Kafka uses a pull model for data consumption, which means that consumers actively request data from Kafka rather than Kafka pushing data to consumers. Imagine if Kafka were to use a push model where machines send tens of thousands of data to Kafka and Kafka immediately forwards it to consumers, the consumers\u0026rsquo; computers would likely crash due to performance issues throughout the day.\nHow the Pull Model Works Producers generate data and send it to Kafka topics. They do not need to know how or when this data will be consumed.\nConsumers subscribe to Kafka topics and pull data from these topics as needed. They send requests to Kafka brokers, asking for new records from specific partitions.\nBenefits of the Pull Model Consumer-Controlled Flow : Consumers control the rate at which they consume data. They can pull data at their own pace, which helps in managing and balancing load.\nEfficient Resource Utilization : By pulling data, consumers can ensure they are ready to process the data, avoiding potential overload or resource contention.\nFlexibility in Processing : Consumers can decide how much data to pull and process in each request, allowing them to adapt to varying workloads and processing capabilities.\nImproved Fault Tolerance : In a pull model, if a consumer fails or needs to restart, it can simply resume pulling data from where it left off, reducing the risk of data loss or duplication.\nUsing Kafka\u0026rsquo;s pull model, these updates are sent to Kafka topics by the machines (producers). Monitoring systems (consumers) then pull this data at their own pace, processing each update in real-time without being overwhelmed by the volume of data.\nKey Roles in Kafka 1. Broker Simply put, a broker is a Kafka node responsible for receiving, storing, and synchronizing data. Each broker not only serves consumers but also communicates with other brokers for data backup, forming a distributed cluster.\n2. Topic In Kafka, topic is like a category where producers send their messages. It acts as a central hub where producers put out data, and consumers sign up to get and use that data.\n3. Producer Producers are responsible for continuously sending data to Kafka topics , with each piece of data considered a record. Producers can be various applications, such as monitoring systems, web server logs, or real-time data from factory machines.\n4. Consumer Consumers can pull \"message\" at their own pace. They can operate independently or as part of a consumer group, dividing consumption tasks among the group members based on partitions.\nConceptual diagram:\n","date":"2023-08-25T00:00:00Z","image":"https://shawnswu.github.io/secondbrain/p/getting-to-know-kafka-2/images/apache-kafka_hu148f63155e1096e2ff1607c98c1189e7_45286_120x120_fill_q75_box_smart1.jpg","permalink":"https://shawnswu.github.io/secondbrain/p/getting-to-know-kafka-2/","title":"Getting to Know Kafka (2)"},{"content":" A single broker in Kafka can handle multiple topics concurrently. Each Topic have the following two concepts\n1. Partition 2. Replica Imagine a McDonald's restaurant, each hamburger represents a piece of data (message) within a Kafka broker. Partition in Kafka are similar to service counters at McDonald\u0026rsquo;s. If there is only one counter and too many customers, the queue can become very long. To improve service efficiency, McDonald's opens multiple service counters (partitions) , allowing customers to spread out across different counters and speed up service.\nProducers in Kafka are like the kitchens in McDonald's . they produce hamburgers (messages) and distribute them evenly across different counters.\nConsumers are like McDonald'scustomers who only need to go to their designated counter to get their bergers.\nCustomers at different counters can be served simultaneously without affecting each other. Increasing the number of counters improves the overall service capacity.\nOn the other hand, some counter at McDonald\u0026rsquo;s may temporarily close for maintenance, in such cases, all customers would need to go to the other okay counters.\nTo handle this situation, McDonald\u0026rsquo;s will back up several『service counter』(Replica) for each『service counter』(Partition).\nThese back up『service counter』 called replicas in Kafka terms. When the main counter (leader replica) closes, one of its backup counters (follower replica) immediately takes over and continues serving customers. This ensures that customers are not affected by the closure of a particular counter.\n1. Partition A topic can be divided into multiple partitions. Partitions as the basic unit of messages, each partition containing an ordered sequence of messages. Physically, each partition is stored on one or more servers to achieve fault tolerance and scalability. It is precisely because of this mechanism (Partition), it allows Kafka clusters to horizontally scale, enhancing message throughput. By distributing messages across multiple partitions.\n2. Replica In Kafka, each partition can have one or more replicas. A replica is a copy of the content within a partition, and replicas are stored on different servers within the Kafka cluster. It is precisely because of the replica mechanism that system reliability and fault tolerance are enhanced.\nWhen a server fails, data can be recovered from replicas stored on other servers. Through the design of Partitions and Replicas, Kafka achieves horizontal scalability, load balancing, and fault tolerance.\nPartitions is the foundation for horizontally scaling message processing Replicas provide backup and automatic failover capabilities. In this way, each Broker in Kafka not only records its own data but also helps replicate data for other Brokers. This redundancy ensures that even if several nodes fail, the system can continue operating normally because other Brokers can take on the responsibility.\nMy thoughts Conceptually not quite hard, but I feel the real core is the consensus algorithm between each node.\n","date":"2023-08-24T00:00:00Z","image":"https://shawnswu.github.io/secondbrain/p/getting-to-know-kafka-3/images/apache-kafka_hu148f63155e1096e2ff1607c98c1189e7_45286_120x120_fill_q75_box_smart1.jpg","permalink":"https://shawnswu.github.io/secondbrain/p/getting-to-know-kafka-3/","title":"Getting to Know Kafka (3)"},{"content":"Stack has built-in support for math typesetting using KaTeX.\nIt\u0026rsquo;s not enabled by default side-wide, but you can enable it for individual posts by adding math: true to the front matter. Or you can enable it side-wide by adding math = true to the params.article section in config.toml.\nInline math This is an inline mathematical expression: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$\n1 $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$ Block math $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n1 2 3 $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$ $$ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi),e^{2 \\pi i \\xi x},d\\xi $$\n1 2 3 $$ f(x) = \\int_{-\\infty}^\\infty\\hat f(\\xi)\\,e^{2 \\pi i \\xi x}\\,d\\xi $$ ","date":"2023-08-24T00:00:00Z","permalink":"https://shawnswu.github.io/secondbrain/p/math-typesetting/","title":"Math Typesetting"}]