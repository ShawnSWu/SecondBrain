<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kafka on Shawn Wu</title><link>https://shawnswu.github.io/secondbrain/categories/kafka/</link><description>Recent content in Kafka on Shawn Wu</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 25 Aug 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://shawnswu.github.io/secondbrain/categories/kafka/index.xml" rel="self" type="application/rss+xml"/><item><title>Getting to Know Kafka (1)</title><link>https://shawnswu.github.io/secondbrain/p/getting-to-know-kafka-1/</link><pubDate>Fri, 25 Aug 2023 00:00:00 +0000</pubDate><guid>https://shawnswu.github.io/secondbrain/p/getting-to-know-kafka-1/</guid><description>&lt;img src="https://shawnswu.github.io/secondbrain/p/getting-to-know-kafka-1/images/apache-kafka.jpg" alt="Featured image of post Getting to Know Kafka (1)" />&lt;blockquote>
&lt;span style="color: #30c947; font-size: 21px;">
&lt;b>Imagine a scenario where we have a large number of machines, and our system needs to monitor the real-time status of each machine, which means the machines need to send data back every minute, or even every second. &lt;/b>
&lt;/span>
&lt;/blockquote>
&lt;p>Sending data through a RESTful API in this situation isn&amp;rsquo;t efficient, So we need a &lt;span style="color: #f76a3b; font-size: 21px;">
&lt;b>Guy&lt;/b>
&lt;/span> that can store all this data, keep it safe, and respond to requests whenever someone wants to query it. so this is why we need Kafka.&lt;/p>
&lt;h2 id="why-we-need-kafka">Why We Need Kafka
&lt;/h2>&lt;ol>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 21px;">
&lt;b>Handling High Throughput of Data&lt;/b>
&lt;/span>&lt;/strong>&lt;/p>
&lt;p>&lt;strong>&lt;span style="color: #ff5e86; font-size: 18px;">
&lt;b>Kafka is designed to handle massive volumes of data with high throughput.&lt;/b>
&lt;/span>&lt;/strong>. If your application generates or processes a large amount of data continuously, Kafka can efficiently manage this data flow without bottlenecks.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 21px;">
&lt;b>Ensuring Data Consistency and Durability&lt;/b>
&lt;/span>&lt;/strong>&lt;br>
Kafka uses a &lt;strong>&lt;span style="color: #ff5e86; font-size: 18px;">
&lt;b>log-based storage mechanism&lt;/b>
&lt;/span>&lt;/strong> that ensures data consistency and durability. This means your data is reliably stored and can be replicated across multiple nodes, preventing data loss even.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 21px;">
&lt;b>Decoupling Data Producers and Consumers&lt;/b>
&lt;/span>&lt;/strong> &lt;br>
One of Kafka’s key strengths is its ability to &lt;strong>&lt;span style="color: #ff5e86; font-size: 18px;">
&lt;b>decouple data producers and consumers&lt;/b>
&lt;/span>&lt;/strong>.&lt;/p>
&lt;ul>
&lt;li>Producers simply send data to Kafka without needing to know who will consume it or how it will be processed.&lt;/li>
&lt;li>Consumers pull data from Kafka as needed, allowing them to process data independently.&lt;/li>
&lt;/ul>
&lt;p>This decoupling enhances system flexibility and makes it easier to scale and maintain.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 21px;">
&lt;b>Real-Time Data Processing&lt;/b>
&lt;/span>&lt;/strong>&lt;br>
Kafka supports real-time data processing, making it ideal for applications that require immediate data analysis and action. For example, :&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Monitoring systems&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Anomaly detection&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Real-time analytics&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>Kafka enables quick and efficient data handling.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 21px;">
&lt;b>High Scalability and Fault Tolerance&lt;/b>
&lt;/span>&lt;/strong> &lt;br>
Kafka’s architecture is inherently scalable. &lt;strong>&lt;span style="color: #ff5e86; font-size: 18px;">
&lt;b>You can add more brokers to handle increased load&lt;/b>
&lt;/span>&lt;/strong>, kafka will automatically balance the data across the cluster.&lt;/p>
&lt;p>Additionally, Kafka’s replication mechanism ensures high availability and fault tolerance, making it a reliable choice for mission-critical applications.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 21px;">
&lt;b>Event Sourcing and Stream Processing&lt;/b>
&lt;/span>&lt;/strong> &lt;br>
Kafka is perfect for event sourcing and stream processing architectures. It allows you to store each event as a record, enabling you to reconstruct the state of your application from these events. &lt;br>
This is particularly useful in microservices architectures where maintaining the state across services is crucial.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="when-you-might-use-kafka">When You Might Use Kafka
&lt;/h2>&lt;ol>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 21px;">
&lt;b>Real-Time Analytics&lt;/b>
&lt;/span>&lt;/strong> &lt;br>
When you need to analyze data in real-time, such as tracking user activity on a website, monitoring system performance, or financial transaction analysis, Kafka provides a robust platform to ingest, process, and analyze data instantly.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 21px;">
&lt;b>Data Integration&lt;/b>
&lt;/span>&lt;/strong> &lt;br>
Kafka is excellent for integrating data from various sources into a single platform. If you have multiple systems generating data, Kafka can aggregate this data in real-time, making it accessible for processing and analysis.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 21px;">
&lt;b>Log Aggregation&lt;/b>
&lt;/span>&lt;/strong> &lt;br>
For applications that generate logs from different services or systems, Kafka can centralize these logs, making it easier to monitor and troubleshoot issues.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 21px;">
&lt;b>Microservices Communication&lt;/b>
&lt;/span>&lt;/strong> &lt;br>
In a microservices architecture, different services need to communicate and share data. Kafka acts as a central hub, facilitating seamless communication between services through its publish-subscribe model.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 21px;">
&lt;b>Data Lakes and Warehousing&lt;/b>
&lt;/span>&lt;/strong> &lt;br>
If you’re building a data lake or warehouse, Kafka can efficiently stream data from various sources into your storage systems, ensuring data is continuously updated and available for analysis.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 21px;">
&lt;b>Message Queuing&lt;/b>
&lt;/span>&lt;/strong> &lt;br>
For applications that require reliable and ordered message delivery, such as task scheduling or asynchronous processing, Kafka provides a durable and scalable message queuing solution.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 21px;">
&lt;b>IoT Data Streams&lt;/b>
&lt;/span>&lt;/strong> &lt;br>
In IoT applications, devices generate continuous data streams that need to be processed in real-time. Kafka’s ability to handle high throughput and real-time processing makes it ideal for IoT data management.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="my-thoughts">My thoughts
&lt;/h2>&lt;p>If your application involves high throughput data ingestion, real-time analytics, data integration, or microservices communication,
Kafka is a solution worth considering. It provides the scalability, fault tolerance, and flexibility needed to handle modern data challenges effectively.&lt;/p></description></item><item><title>Getting to Know Kafka (2)</title><link>https://shawnswu.github.io/secondbrain/p/getting-to-know-kafka-2/</link><pubDate>Fri, 25 Aug 2023 00:00:00 +0000</pubDate><guid>https://shawnswu.github.io/secondbrain/p/getting-to-know-kafka-2/</guid><description>&lt;img src="https://shawnswu.github.io/secondbrain/p/getting-to-know-kafka-2/images/apache-kafka.jpg" alt="Featured image of post Getting to Know Kafka (2)" />&lt;h2 id="what-are-kafka-characteristic">What are Kafka characteristic?
&lt;/h2>&lt;ol>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 18px;">
&lt;b>Horizontal Scalability and High Availability&lt;/b>
&lt;/span>&lt;/strong>&lt;br>
Achieved through partitioning and replication mechanisms.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 18px;">
&lt;b>Data Consistency&lt;/b>
&lt;/span>&lt;/strong>: Ensured by using a log mechanism for data persistence and replication.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 18px;">
&lt;b>Separation of Data Storage and Consumption&lt;/b>
&lt;/span>&lt;/strong>: Producers generate data while consumers read and process data independently, enhancing system flexibility.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 18px;">
&lt;b>Decoupling Producers and Consumers&lt;/b>
&lt;/span>&lt;/strong>: Producers send data to Kafka without worrying about the consumers. Consumers pull data from Kafka as needed, known as the pull model, which decouples the dependency between the two and increases system flexibility.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h2 id="kafka-is-pull-model">Kafka is Pull Model
&lt;/h2>&lt;p>Kafka uses a pull model for data consumption, &lt;span style="color: #ff8247; font-size: 18px;">
&lt;b>which means that consumers actively request data from Kafka rather than Kafka pushing data to consumers.&lt;/b>
&lt;/span>&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Imagine if Kafka were to use a push model where machines send tens of thousands of data to Kafka and Kafka immediately forwards it to consumers, &lt;br>
the consumers&amp;rsquo; computers would likely crash due to performance issues throughout the day.&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;h3 id="how-the-pull-model-works">How the Pull Model Works
&lt;/h3>&lt;ol>
&lt;li>
&lt;p>Producers generate data and send it to Kafka topics. They do not need to know how or when this data will be consumed.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Consumers subscribe to Kafka topics and pull data from these topics as needed. They send requests to Kafka brokers, asking for new records from specific partitions.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="benefits-of-the-pull-model">Benefits of the Pull Model
&lt;/h3>&lt;ol>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 19px;">
&lt;b>Consumer-Controlled Flow&lt;/b>
&lt;/span>&lt;/strong>: Consumers control the rate at which they consume data. They can pull data at their own pace, which helps in managing and balancing load.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 19px;">
&lt;b>Efficient Resource Utilization&lt;/b>
&lt;/span>&lt;/strong>: By pulling data, consumers can ensure they are ready to process the data, avoiding potential overload or resource contention.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 19px;">
&lt;b>Flexibility in Processing&lt;/b>
&lt;/span>&lt;/strong>: Consumers can decide how much data to pull and process in each request, allowing them to adapt to varying workloads and processing capabilities.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>&lt;span style="color: #E6CC93; font-size: 19px;">
&lt;b>Improved Fault Tolerance&lt;/b>
&lt;/span>&lt;/strong>: In a pull model, if a consumer fails or needs to restart, it can simply resume pulling data from where it left off, reducing the risk of data loss or duplication.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Using Kafka&amp;rsquo;s pull model, these updates are sent to Kafka topics by the machines (producers). Monitoring systems (consumers) then pull this data at their own pace, processing each update in real-time without being overwhelmed by the volume of data.&lt;/p>
&lt;p>&lt;img src="https://shawnswu.github.io/secondbrain/secondbrain/p/getting-to-know-kafka-2/images/1.png"
width="473"
height="333"
srcset="https://shawnswu.github.io/secondbrain/secondbrain/p/getting-to-know-kafka-2/images/1_hu57f8cb03b8667c102b50cfc92862d0af_34083_480x0_resize_box_3.png 480w, https://shawnswu.github.io/secondbrain/secondbrain/p/getting-to-know-kafka-2/images/1_hu57f8cb03b8667c102b50cfc92862d0af_34083_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Kafka Diagram 1"
class="gallery-image"
data-flex-grow="142"
data-flex-basis="340px"
>&lt;/p>
&lt;h2 id="key-roles-in-kafka">Key Roles in Kafka
&lt;/h2>&lt;h3 id="1-broker">1. &lt;strong>Broker&lt;/strong>
&lt;/h3>&lt;p>Simply put, a broker is a Kafka node responsible for receiving, storing, and synchronizing data. Each broker not only serves consumers but also communicates with other brokers for data backup, forming a distributed cluster.&lt;/p>
&lt;h3 id="2-topic">2. &lt;strong>Topic&lt;/strong>
&lt;/h3>&lt;p>In Kafka, &lt;span style="color: #E6CC93; font-size: 19px;">
&lt;b>topic is like a category where producers send their messages.&lt;/b>
&lt;/span> It acts as a central hub where producers put out data, and consumers sign up to get and use that data.&lt;/p>
&lt;h3 id="3-producer">3. &lt;strong>Producer&lt;/strong>
&lt;/h3>&lt;p>&lt;span style="color: #E6CC93; font-size: 19px;">
&lt;b>Producers are responsible for continuously sending data to Kafka topics&lt;/b>
&lt;/span>, with each piece of data considered a record. Producers can be various applications, such as monitoring systems, web server logs, or real-time data from factory machines.&lt;/p>
&lt;h3 id="4-consumer">4. &lt;strong>Consumer&lt;/strong>
&lt;/h3>&lt;p>&lt;span style="color: #E6CC93; font-size: 19px;">
&lt;b>Consumers can pull "message" at their own pace.&lt;/b>
&lt;/span> They can operate independently or as part of a consumer group, dividing consumption tasks among the group members based on partitions.&lt;/p>
&lt;p>Conceptual diagram:&lt;/p>
&lt;p>&lt;img src="https://shawnswu.github.io/secondbrain/secondbrain/p/getting-to-know-kafka-2/images/2.png"
width="500"
height="300"
srcset="https://shawnswu.github.io/secondbrain/secondbrain/p/getting-to-know-kafka-2/images/2_hu6b745f9803af8886120d5e383f6c6b91_34719_480x0_resize_box_3.png 480w, https://shawnswu.github.io/secondbrain/secondbrain/p/getting-to-know-kafka-2/images/2_hu6b745f9803af8886120d5e383f6c6b91_34719_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Kafka Diagram 2"
class="gallery-image"
data-flex-grow="166"
data-flex-basis="400px"
>&lt;/p></description></item><item><title>Getting to Know Kafka (3)</title><link>https://shawnswu.github.io/secondbrain/p/getting-to-know-kafka-3/</link><pubDate>Thu, 24 Aug 2023 00:00:00 +0000</pubDate><guid>https://shawnswu.github.io/secondbrain/p/getting-to-know-kafka-3/</guid><description>&lt;img src="https://shawnswu.github.io/secondbrain/p/getting-to-know-kafka-3/images/apache-kafka.jpg" alt="Featured image of post Getting to Know Kafka (3)" />&lt;h1 id="hahahugoshortcode9s0hbhb">&lt;strong>&lt;span style="color: #E6CC93; font-size: 25px;">
&lt;b>A single broker in Kafka can handle multiple topics concurrently.&lt;/b>
&lt;/span>&lt;/strong>
&lt;/h1>&lt;p>&lt;strong>Each Topic have the following two concepts&lt;/strong>&lt;/p>
&lt;p>&lt;span style="color: #82f571; font-size: 20px;">
&lt;b>1. Partition&lt;/b>
&lt;/span> &lt;br>
&lt;span style="color: #82f571; font-size: 20px;">
&lt;b>2. Replica&lt;/b>
&lt;/span>&lt;/p>
&lt;blockquote>
&lt;span style="color: #E6CC93; font-size: 20px;">
&lt;b>Imagine a McDonald's restaurant, each hamburger represents a piece of data (message) within a Kafka broker.&lt;/b>
&lt;/span>
&lt;/blockquote>
&lt;p>&lt;span style="color: #82f571; font-size: 20px;">
&lt;b>Partition&lt;/b>
&lt;/span> in Kafka are similar to &lt;span style="color: #ff2e74; font-size: 17px;">
&lt;b>service counters&lt;/b>
&lt;/span> at McDonald&amp;rsquo;s. If there is only one counter and too many customers, the queue can become very long.
To improve service efficiency, &lt;span style="color: #ff2e74; font-size: 17px;">
&lt;b>McDonald's opens multiple service counters (partitions)&lt;/b>
&lt;/span> , allowing customers to spread out across different counters and speed up service.&lt;/p>
&lt;p>&lt;span style="color: #E6CC93; font-size: 18px;">
&lt;b>Producers&lt;/b>
&lt;/span> in Kafka are &lt;span style="color: #ff2e74; font-size: 17px;">
&lt;b>like the kitchens in McDonald's&lt;/b>
&lt;/span>.
they produce hamburgers (messages) and distribute them evenly across different counters.&lt;/p>
&lt;p>&lt;span style="color: #E6CC93; font-size: 18px;">
&lt;b>Consumers&lt;/b>
&lt;/span> are &lt;span style="color: #ff2e74; font-size: 17px;">
&lt;b>like McDonald'scustomers&lt;/b>
&lt;/span> who only need to go to their designated counter to get their bergers.&lt;/p>
&lt;p>Customers at different counters can be served simultaneously without affecting each other. Increasing the number of counters improves the overall service capacity.&lt;/p>
&lt;p>On the other hand, some counter at McDonald&amp;rsquo;s may temporarily close for maintenance, in such cases, all customers would need to go to the other okay counters.&lt;/p>
&lt;p>To handle this situation, McDonald&amp;rsquo;s will back up several『service counter』(Replica) for each『service counter』(Partition).&lt;/p>
&lt;span style="color: #82f571; font-size: 16px;">
&lt;b>These back up『service counter』 called replicas in Kafka terms.&lt;/b>
&lt;/span>
&lt;p>When the main counter (leader replica) closes, one of its backup counters (follower replica) immediately takes over and continues serving customers. This ensures that customers are not affected by the closure of a particular counter.&lt;/p>
&lt;h3 id="1-partition">1. Partition
&lt;/h3>&lt;ul>
&lt;li>&lt;span style="color: #E6CC93; font-size: 17px;">
&lt;b>A topic can be divided into multiple partitions.&lt;/b>
&lt;/span>&lt;/li>
&lt;li>&lt;span style="color: #E6CC93; font-size: 17px;">
&lt;b>Partitions as the basic unit of messages, each partition containing an ordered sequence of messages.&lt;/b>
&lt;/span>&lt;/li>
&lt;li>&lt;span style="color: #E6CC93; font-size: 17px;">
&lt;b>Physically, each partition is stored on one or more servers to achieve fault tolerance and scalability.&lt;/b>
&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>It is precisely because of this mechanism (Partition), it allows Kafka clusters to horizontally scale, enhancing message throughput. By distributing messages across multiple partitions.&lt;/p>
&lt;h3 id="2-replica">2. Replica
&lt;/h3>&lt;ul>
&lt;li>&lt;span style="color: #E6CC93; font-size: 17px;">
&lt;b>In Kafka, each partition can have one or more replicas.&lt;/b>
&lt;/span>&lt;/li>
&lt;li>&lt;span style="color: #E6CC93; font-size: 17px;">
&lt;b>A replica is a copy of the content within a partition, and replicas are stored on different servers within the Kafka cluster.&lt;/b>
&lt;/span>&lt;/li>
&lt;/ul>
&lt;p>It is precisely because of the replica mechanism that system reliability and fault tolerance are enhanced.&lt;/p>
&lt;span style="color: #ff2e74; font-size: 22px;">
&lt;b>When a server fails, data can be recovered from replicas stored on other servers.&lt;/b>
&lt;/span>
&lt;p>Through the design of Partitions and Replicas, Kafka achieves horizontal scalability, load balancing, and fault tolerance.&lt;/p>
&lt;ul>
&lt;li>Partitions is the foundation for horizontally scaling message processing&lt;/li>
&lt;li>Replicas provide backup and automatic failover capabilities.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>In this way, each Broker in Kafka not only records its own data but also helps replicate data for other Brokers.
This redundancy ensures that even if several nodes fail, the system can continue operating normally because other Brokers can take on the responsibility.&lt;/p>
&lt;/blockquote>
&lt;figure>&lt;img src="https://shawnswu.github.io/secondbrain/secondbrain/p/getting-to-know-kafka-3/images/1.png" width="600" height="500">
&lt;/figure>
&lt;h3 id="my-thoughts">My thoughts
&lt;/h3>&lt;p>Conceptually not quite hard, but I feel the real core is the consensus algorithm between each node.&lt;/p></description></item></channel></rss>